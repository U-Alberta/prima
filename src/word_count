#!/usr/bin/python
import os
import sys
import linecache
import nltk.tokenize
from nltk.tokenize import sent_tokenize, word_tokenize

def name_outfile(path):
	outfile = "word_count_"
	path = path.split("/")
	if len(path) > 2: # if there is a filename in the path, strip it of the type identifier
		outfile+="_".join(path[:2])
		outfile+="_"+path[2].split(".")[0]
		if len(path) > 3:
			outfile+="_"+path[3]
	else:
		outfile+="_".join(path)
	outfile = "processed/word_count/"+outfile
	if not os.path.exists("processed/word_count/"):
		os.makedirs("processed/word_count")
	return outfile

def count_collection(path):
	n = 0
	for doc_path in os.listdir(path):
		n+=count_item(path+"/"+doc_path)
	return n

def count_item(path):
	n = 0
	for doc_path in os.listdir(path):
		n+=count_file(path+"/"+doc_path)
	return n

def count_file(path):
  n = 0
  doc = open(path, "r")
  for line in doc:
    n+=count_line(line)
  doc.close()
  return n

def count_line(line):
  punc = [".", ",", ";", ":", "!", "?", "-", "_", "'", '"', "'s"]
  n = 0
  sentence_list = sent_tokenize(line.decode("utf-8"))
  for sentence in sentence_list:
    for term in word_tokenize(sentence):
      if term not in punc:
        n+=1
  return n

def main():
	if len(sys.argv) != 2:
		print("Invalid number of command line arguments")
		return -1
	path = sys.argv[1]
	depth = len(path.split("/"))
	if depth == 1:
		n = count_collection(path)
	elif depth == 2:
		n = count_item(path)
	elif depth == 3:
		n = count_file(path)
	elif depth == 4:
		lineno = int(path.split("/")[-1])
		tmp = path.split("/")[:-1]
		newpath = "/".join(tmp)
		line = linecache.getline(newpath, lineno)
		n = count_line(line)
	else:
		print("Invalid word count depth")
		return -1
	n = str(n)
	outfilename = name_outfile(path)
	outfile = open(outfilename, "w")
	outfile.write(n)
	outfile.close()
	return n

main()